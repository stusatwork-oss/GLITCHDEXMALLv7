# PHOTO ASSETS - 3-LAYER SEMANTIC SORT

**Purpose:** AI-native photo organization for semantic understanding, canon building, and Sora integration.

---

## ğŸ¯ The 3-Layer Sort Method

This structure enables Claude 4.5 and GitHub GPT to understand:
- **What** these assets mean
- **How** they relate
- **How** to reference them in canon
- **How** to use them in Sora prompts
- **How** to build consistent world logic

---

## Layer 1: PHYSICAL ZONES (Surface-Level Grouping)

**Purpose:** Basic spatial classification

Use ONLY when the file clearly belongs to a physical place:

```
zones/
â”œâ”€â”€ escalators/        # Escalator photos (primary scale calibration)
â”œâ”€â”€ food_court/        # Sunken food court (including pit)
â”œâ”€â”€ movie_mouth/       # Movie theater entrance area
â”œâ”€â”€ comphut/           # Computer store area
â”œâ”€â”€ maintenance/       # Service corridors, back areas
â”œâ”€â”€ atrium/            # Main atrium and fountain
â””â”€â”€ exterior/          # Parking lot, building exterior
```

**Guidelines:**
- Only use if photo clearly shows a specific zone
- If ambiguous, use Layer 2 (semantic) instead
- Cross-reference with `v6-nextgen/canon/zones/`

---

## Layer 2: FUNCTIONAL SEMANTIC ZONES (AI Magic Layer) â­

**Purpose:** Meaning-based classification for AI understanding

This is THE MOST IMPORTANT layer. It tells AI what photos represent conceptually.

```
semantic/
â”œâ”€â”€ architectural_features/     # Columns, masts, tensile roof, structural
â”œâ”€â”€ lighting_conditions/        # Natural light, fluorescent, dim, shadows
â”œâ”€â”€ material_patterns/          # Glass block, tile, carpet, concrete
â”œâ”€â”€ signage_and_wayfinding/     # Directional signs, store signs, maps
â”œâ”€â”€ reflections_and_mirrors/    # Reflective surfaces, glass, water
â”œâ”€â”€ storefronts_closed/         # Shuttered stores, vacant spaces
â”œâ”€â”€ storefronts_open/           # Active retail, displays
â”œâ”€â”€ abandoned_elements/         # Decay, neglect, deterioration
â””â”€â”€ flooring_patterns/          # Floor materials, patterns, wear
```

**Why This Matters:**

AI uses these categories to:
- âœ“ Write canon documents
- âœ“ Define zone characteristics
- âœ“ Build environmental logic
- âœ“ Understand Cloud-level moods
- âœ“ Detect aesthetic rules
- âœ“ Recognize contradictions
- âœ“ Place NPCs correctly
- âœ“ Write Sora 3-anchor location guides

**Classification Rules:**
1. Photo can be in MULTIPLE semantic categories
2. This is an "index of meaning," not just an index of images
3. Cross-reference with `/ai/sora/` prompt templates
4. Links to `ai/mallOS/` environmental state

---

## Layer 3: STORY/CHARACTER ZONES (Latent AI Grouping)

**Purpose:** Narrative and mood classification for MallOS/Cloud integration

Connects to SPYNT character spines and MallOS Cloud states:

```
narrative/
â”œâ”€â”€ pov_shots/              # First-person perspective photos
â”œâ”€â”€ human_scale/            # Photos showing scale via people/objects
â”œâ”€â”€ mood_low_cloud/         # Calm, peaceful, optimistic (pressure 0-33)
â”œâ”€â”€ mood_mid_cloud/         # Neutral, browsing, wandering (pressure 34-66)
â”œâ”€â”€ mood_high_cloud/        # Tense, abandoned, eerie (pressure 67-100)
â”œâ”€â”€ liminal/                # Transitional spaces, thresholds, emptiness
â””â”€â”€ glitch_candidates/      # Photos showing contradictions, anomalies
```

**Integration:**

**MallOS Cloud States:**
- `mood_low_cloud/` â†’ Cloud pressure 0-33 (TENSION, WANDER moods)
- `mood_mid_cloud/` â†’ Cloud pressure 34-66 (WANDER, SURGE moods)
- `mood_high_cloud/` â†’ Cloud pressure 67-100 (SURGE, BLEED moods)

**SPYNT Character Integration:**
- Use `human_scale/` for character placement logic
- Use `pov_shots/` for first-person narrative moments
- Use `liminal/` for character transition scenes

**Sora Prompt Generation:**
- Reference `glitch_candidates/` for bleed event visuals
- Use mood folders for tone matching
- Cross-reference with `ai/sora/shot_logic/cloud_mood_mapping.json`

---

## ğŸ“ Current Contents

### eastland-archive/
**Status:** 153 photos from Flickr/community archives
**Next:** Sort into 3-layer structure using classification script

**Contents:**
- Historical photos (1981-2011)
- Various eras and conditions
- Mixed quality and perspectives
- Includes EXIF data (some photos)

---

## ğŸ”§ Classification Workflow

### Step 1: Run Batch Classifier
```bash
python ai/pipelines/photo_processing/batch_classify.py \
  --input v6-nextgen/assets/photos/eastland-archive/ \
  --output classification_results.csv \
  --layers all
```

### Step 2: Review Classifications
- Check CSV for:
  - Layer 1 (zone) assignments
  - Layer 2 (semantic) tags (multiple allowed)
  - Layer 3 (narrative/mood) tags

### Step 3: Symlink Photos
```bash
python ai/pipelines/photo_processing/create_symlinks.py \
  --classification classification_results.csv \
  --source eastland-archive/ \
  --target zones/ semantic/ narrative/
```

### Step 4: Validate
```bash
python ai/pipelines/validation/validate_photo_structure.py
```

---

## ğŸ¯ Classification Guidelines

### Layer 1 (Physical Zones)
**Ask:** "Where IS this?"
- If clear: Put in appropriate zone folder
- If ambiguous: Skip Layer 1, use Layer 2 instead

### Layer 2 (Semantic) â­
**Ask:** "What does this SHOW?" (multiple answers OK)
- Structural elements? â†’ `architectural_features/`
- Light quality? â†’ `lighting_conditions/`
- Material close-up? â†’ `material_patterns/`
- Signs visible? â†’ `signage_and_wayfinding/`
- Reflections? â†’ `reflections_and_mirrors/`
- Store status? â†’ `storefronts_closed/` or `storefronts_open/`
- Decay/neglect? â†’ `abandoned_elements/`
- Floor visible? â†’ `flooring_patterns/`

### Layer 3 (Narrative)
**Ask:** "What MOOD or STORY does this evoke?"
- POV angle? â†’ `pov_shots/`
- People/scale reference? â†’ `human_scale/`
- What Cloud pressure? â†’ `mood_low/mid/high_cloud/`
- Transitional/empty? â†’ `liminal/`
- Contradiction/anomaly? â†’ `glitch_candidates/`

---

## ğŸ”— Integration Points

### With AI Tooling
- **`ai/sora/templates/`** - Reference semantic categories in prompts
- **`ai/mallOS/zones/`** - Link zone photos to simulation zones
- **`ai/spynt/`** - Use narrative photos for character context
- **`ai/renderist/`** - Pull from mood categories for lore

### With Canon
- **`v6-nextgen/canon/zones/`** - Each zone references Layer 1 photos
- **`v6-nextgen/canon/characters/`** - Characters link to narrative photos
- **`v6-nextgen/docs/reference/`** - Technical docs reference semantic photos

### With Sora
- **Prompt templates** use semantic categories:
  ```
  [LIGHTING] lighting_conditions/natural_sunlight_12.jpg
  [MATERIALS] material_patterns/glass_block_wall_04.jpg
  [MOOD] mood_high_cloud/abandoned_corridor_08.jpg
  ```

---

## ğŸ“Š Statistics (After Classification)

Will track:
- Total photos per Layer 1 zone
- Distribution across Layer 2 semantic categories
- Mood breakdown (Layer 3)
- Multi-tagged photos (photos in 2+ categories)
- Coverage gaps (missing semantic types)

---

## ğŸš€ Next Steps

1. â³ **Classify eastland-archive/** photos
2. â³ Create symlinks to 3-layer structure
3. â³ Validate coverage (check for gaps)
4. â³ Document semantic patterns
5. â³ Integrate with ai/sora/ templates
6. â³ Link to canon/ definitions

---

## ğŸ“ Notes

**Why Symlinks?**
- Photos stay in `eastland-archive/` (preservation)
- Multiple layers can reference same photo
- Easy to adjust classifications

**Multi-Category Photos:**
A single photo can be:
- Layer 1: `zones/atrium/`
- Layer 2: `semantic/architectural_features/` + `semantic/lighting_conditions/`
- Layer 3: `narrative/mood_mid_cloud/`

This is intentional and encouraged.

**Automation:**
The `ai/pipelines/` scripts will handle:
- Batch classification
- Symlink creation
- Validation
- Gap detection

---

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                â•‘
â•‘  3-LAYER SORT = AI-NATIVE SEMANTIC UNDERSTANDING               â•‘
â•‘                                                                â•‘
â•‘  Layer 1: WHERE (physical zones)                              â•‘
â•‘  Layer 2: WHAT (semantic meaning) â­ MOST IMPORTANT           â•‘
â•‘  Layer 3: MOOD (narrative/Cloud states)                       â•‘
â•‘                                                                â•‘
â•‘  This gives AI an index of MEANING, not just images.          â•‘
â•‘                                                                â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

*Structure ready for classification.*
*Run batch classifier when ready to sort 153 photos.*
